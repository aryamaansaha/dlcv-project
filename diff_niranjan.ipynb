{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=mnist_transform)\n",
    "mnist_testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=mnist_transform)\n",
    "\n",
    "svhn_transform = transforms.Compose([transforms.ToTensor(), ])\n",
    "svhn_trainset = torchvision.datasets.SVHN(root='./data', split='train', download=True, transform=svhn_transform)\n",
    "svhn_testset = torchvision.datasets.SVHN(root='./data', split='test', download=True, transform=svhn_transform)\n",
    "\n",
    "# # Load pre-trained ResNet and modify for feature extraction\n",
    "# resnet = resnet18(pretrained=False, num_classes=10)\n",
    "# resnet.load_state_dict(torch.load(\"path_to_mnist_resnet.pth\"))\n",
    "# resnet.eval()\n",
    "\n",
    "# # Feature extraction function\n",
    "# def extract_features(image_batch):\n",
    "#     with torch.no_grad():\n",
    "#         features = resnet(image_batch)\n",
    "#     return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel, DDPMScheduler\n",
    "\n",
    "# Define UNet for diffusion\n",
    "unet = UNet2DConditionModel(\n",
    "    sample_size=32,  # SVHN image size\n",
    "    in_channels=3,  # RGB for SVHN\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 128, 256),\n",
    "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"UpBlock2D\", \"AttnUpBlock2D\"),\n",
    "    cross_attention_dim=512  # Conditioning vector size\n",
    ").to(device)\n",
    "\n",
    "# Define noise scheduler\n",
    "scheduler = DDPMScheduler(num_train_timesteps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedMNISTSVHN(Dataset):\n",
    "    def __init__(self, mnist_dataset, svhn_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.svhn_dataset = svhn_dataset\n",
    "\n",
    "        # Create a mapping from digit labels to indices for SVHN\n",
    "        self.svhn_label_to_indices = {i: [] for i in range(10)}\n",
    "        for idx, label in enumerate(self.svhn_dataset.labels):\n",
    "            self.svhn_label_to_indices[label].append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the MNIST dataset\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get an MNIST sample\n",
    "        mnist_img, mnist_label = self.mnist_dataset[idx]\n",
    "        \n",
    "        # Get a random SVHN sample with the same label\n",
    "        svhn_idx = torch.randint(0, len(self.svhn_label_to_indices[mnist_label]), (1,)).item()\n",
    "        svhn_img = self.svhn_dataset[self.svhn_label_to_indices[mnist_label][svhn_idx]][0]\n",
    "\n",
    "        return mnist_img, svhn_img, mnist_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired Dataset\n",
    "paired_trainset = PairedMNISTSVHN(mnist_trainset, svhn_trainset)\n",
    "paired_trainloader = DataLoader(paired_trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Paired Dataset\n",
    "paired_testset = PairedMNISTSVHN(mnist_testset, svhn_testset)\n",
    "paired_testloader = DataLoader(paired_testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class ResNetFeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, original_resnet):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        # Retain all layers except the final FC layer\n",
    "        self.features = torch.nn.Sequential(*list(original_resnet.children())[:-1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "# Load the ResNet model and modify it\n",
    "resnet = resnet18(pretrained=False)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = torch.nn.Linear(num_ftrs, 10) # MNIST has 10 classes\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "resnet.load_state_dict(torch.load(\"resnet_model.pth\"))\n",
    "feature_extractor = ResNetFeatureExtractor(resnet)\n",
    "\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "feature_extractor = feature_extractor.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# # Pass an input through the modified model\n",
    "# input_image = torch.randn(1, 3, 224, 224)  # Example input\n",
    "# features = feature_extractor(input_image)\n",
    "\n",
    "# print(\"Extracted Features Shape:\", features.shape)  # [batch_size, 512, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/1875 [01:38<2:18:13,  4.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(paired_trainloader):\n\u001b[0;32m     13\u001b[0m     mnist_images, svhn_images, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 14\u001b[0m     mnist_images \u001b[38;5;241m=\u001b[39m \u001b[43mmnist_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     svhn_images \u001b[38;5;241m=\u001b[39m svhn_images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import SVHN\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 5\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(paired_trainloader):\n",
    "        mnist_images, svhn_images, labels = batch\n",
    "        mnist_images = mnist_images.to(device)\n",
    "        svhn_images = svhn_images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Extract features from MNIST ResNet\n",
    "        mnist_features = feature_extractor(mnist_images).to(device)\n",
    "\n",
    "        # Generate noise and corrupted images\n",
    "        noise = torch.randn_like(svhn_images).to(device)\n",
    "        timesteps = torch.randint(0, scheduler.num_train_timesteps, (svhn_images.size(0),), device=device).long()\n",
    "        noisy_images = scheduler.add_noise(svhn_images, noise, timesteps)\n",
    "\n",
    "        # Predict noise with UNet conditioned on MNIST features\n",
    "        predicted_noise = unet(noisy_images, timesteps, encoder_hidden_states=mnist_features.squeeze().unsqueeze(1)).sample\n",
    "\n",
    "        # Calculate loss and backpropagate\n",
    "        loss = criterion(predicted_noise, noise)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        torch.save(unet.state_dict(), f\"models/unet_{epoch+1}_model.pth\")\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
